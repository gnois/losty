-- adapted from resty.limit.req
-- conforms to class:incoming() to be used in resty.limit.traffic

var _M = {
	_VERSION = '0.01'
}
var mt = {
	__index = _M
}


-- initial - initial exponential, 2^(n+initial), where n = total requests before ttl, must be +ve
-- ttl - duration before backoff is reset, must be > 0, else delay grows exponentially large
_M.new = \dict_name, initial, ttl ->
	var dict = ngx.shared[dict_name]
	if not dict
		return nil, "shared dict not found"
	assert(initial >= 0 and ttl > 0)

	var @ = {
		dict = dict
		, initial = initial
		, ttl = ttl
	}
	return setmetatable(@, mt)


-- sees an new incoming event
-- the "commit" argument controls whether should we record the event in shm.
-- FIXME we have a (small) race-condition window between dict:get() and
-- dict:set() across multiple nginx worker processes. The size of the
-- window is proportional to the number of workers.
_M.incoming = \@, key, commit ->
	var dict = @.dict
	var now = ngx.now() * 1000

	var delay = 0
	var last, count = dict.get(@, key)
	if last and count
		var elapsed = now - tonumber(last)
		var exp = math.pow(2, count + @.initial) * 1000
		var wait = exp + math.random(1, math.ceil(exp / 3))  -- add (1/3 * exp) jitter
		delay = wait - elapsed
		--print('====== count: ', count, ' exp: ', exp, ' wait: ', wait, ' elapsed: ', elapsed, '  delay: ', delay)

	if commit
		if not count
			count = 1
		else
			count = count + 1
		-- update last hit time, so there will never be 0 delay for more than once
		last = now
		-- ttl keeps being refreshed
		dict.set(@, key, last, @.ttl, count)

	-- return the delay in seconds
	return delay / 1000, count



_M.uncommit = \@, key ->
	assert(key)
	var dict = @.dict

	var last, count = dict.get(@, key)
	if last and count
		if count > 1
			count = count - 1
		-- retain ttl
		var ttl = dict.ttl(@, key)
		dict.set(@, key, last, ttl, count)
		return true
	return nil, "not found"


return _M
